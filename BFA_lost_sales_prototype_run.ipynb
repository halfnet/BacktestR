{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BFA_lost_sales_prototype_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halfnet/BacktestR/blob/master/BFA_lost_sales_prototype_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeF8O47VPaNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8acff3f0-53f6-4e7b-9a51-44bd8036acc1"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqXLL-ssRBXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas_gbq\n",
        "import datetime\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubqzp3z-Cf85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bleed_down_hypothetical(df):\n",
        "  manufacturer_part_id = -1\n",
        "  for index, row in df.iterrows():\n",
        "    if row['manufacturer_part_id'] == manufacturer_part_id:\n",
        "      df.at[index, 'start_qty'] = next_start_qty\n",
        "      df.at[index, 'start_qty2'] = next_start_qty2\n",
        "    else:\n",
        "      manufacturer_part_id = row['manufacturer_part_id']\n",
        "      next_start_qty = row['start_qty']\n",
        "      next_start_qty2 = row['start_qty2']\n",
        "\n",
        "    if (row['pickable_qty'] == 0) & (row['order_qty'] == 0):\n",
        "      # oos and no order at all, use lost_sales to bleed down\n",
        "      next_start_qty = next_start_qty + row['other_receipt_qty'] + row['hypothetical_qty'] - row['potential_lost_sales']\n",
        "      next_start_qty2 = next_start_qty2 + row['other_receipt_qty'] + row['hypothetical_qty2'] - row['potential_lost_sales']\n",
        "      # available qty can't be negative\n",
        "      if next_start_qty < 0:\n",
        "        df.at[index, 'order_qty'] = next_start_qty + row['potential_lost_sales']\n",
        "        next_start_qty = 0\n",
        "      else:\n",
        "        df.at[index, 'order_qty'] = row['potential_lost_sales']\n",
        "\n",
        "      if next_start_qty2 < 0:\n",
        "        df.at[index, 'order_qty2'] = next_start_qty2 + row['potential_lost_sales']\n",
        "        next_start_qty2 = 0\n",
        "      else:\n",
        "        df.at[index, 'order_qty2'] = row['potential_lost_sales']\n",
        "\n",
        "    else:\n",
        "      next_start_qty = next_start_qty + row['other_receipt_qty'] + row['hypothetical_qty'] - row['order_qty']\n",
        "      next_start_qty2 = next_start_qty2 + row['other_receipt_qty'] + row['hypothetical_qty2'] - row['order_qty2']\n",
        "      if next_start_qty < 0:\n",
        "        df.at[index, 'order_qty'] = next_start_qty + row['order_qty']\n",
        "        next_start_qty = 0\n",
        "\n",
        "      if next_start_qty2 < 0:\n",
        "        df.at[index, 'order_qty2'] = next_start_qty2 + row['order_qty2']\n",
        "        next_start_qty2 = 0\n",
        "\n",
        "  # separate hypothetical cases into two dataframes and then stack them together\n",
        "  df2 = df.drop(columns=['start_qty', 'hypothetical_qty', 'order_qty'])\n",
        "  df2.rename(columns={'start_qty2': 'start_qty', 'hypothetical_qty2': 'hypothetical_qty', 'order_qty2': 'order_qty'}, inplace=True)\n",
        "  df2['analysis_case'] = 'hypothetical2'\n",
        "  df.drop(columns=['start_qty2', 'hypothetical_qty2', 'order_qty2'], inplace=True)\n",
        "  df['analysis_case'] = 'hypothetical1'\n",
        "  df = pd.concat([df, df2])\n",
        "\n",
        "  df['is_oos'] = np.where((df.start_qty + df.other_receipt_qty + df.hypothetical_qty - df.order_qty) <= 0, 1, 0)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaMSFZMuq6Is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bleed_down_model(df):\n",
        "  manufacturer_part_id = -1\n",
        "  for index, row in df.iterrows():\n",
        "    if row['manufacturer_part_id'] == manufacturer_part_id:\n",
        "      df.at[index, 'start_qty'] = next_start_qty\n",
        "    else:\n",
        "      manufacturer_part_id = row['manufacturer_part_id']\n",
        "      next_start_qty = row['start_qty']\n",
        "    next_start_qty = next_start_qty + row['inbound_qty'] + row['proposal_qty'] - row['forecast_qty']\n",
        "    next_start_qty = 0 if next_start_qty < 0 else next_start_qty\n",
        "\n",
        "  df['is_oos'] = np.where((df.start_qty + df.inbound_qty + df.proposal_qty - df.forecast_qty) <= 0, 1, 0)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obsPVfMAspGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_temp_table_from_df(project_id, df):\n",
        "  now = datetime.datetime.now()\n",
        "  now = now.strftime(\"%Y%m%d%H%M%S\")\n",
        "  tmp_table_name = \"tmp_lost_sales_bleeddown_\" + now\n",
        "  df.to_gbq(destination_table=dataset+\".\"+tmp_table_name, project_id=project_id, if_exists=\"replace\")\n",
        "  fq_tmp_table_name = \"`\" + project_id + \".\" + dataset + \".\" + tmp_table_name + \"`\"\n",
        "\n",
        "  return tmp_table_name, fq_tmp_table_name"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zOsMVwTt2g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drop_temp_table(client, dataset, tmp_table_name):\n",
        "  sql =\"\"\"\\\n",
        "    DROP TABLE IF EXISTS {}.{};\n",
        "  \"\"\"\n",
        "  query=sql.format(dataset, tmp_table_name)\n",
        "  df = client.query(query).to_dataframe()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BRhhwaGHDUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prototype_run(client, project_id, proposal_id):\n",
        "\n",
        "  print(\"-------\" + str(proposal_id) + \"-------\")\n",
        "\n",
        "  sql =\"\"\"\\\n",
        "      SELECT \n",
        "        MAX(proposal.snapshot_id) snapshot_id,\n",
        "        MAX(DATE(refresh.brstarttime)) refresh_date, \n",
        "        MAX(proposal.order_cadence) order_cadence,\n",
        "        MAX(proposal.arrival_date) arrival_date\n",
        "      FROM `wf-gcp-us-ae-ops-prod.buyfair_stream.vw_buyfair_supply_proposal` proposal\n",
        "      INNER JOIN `wf-gcp-us-ae-ops-prod.buyfair_bulk.tbl_buyfair_refresh_vertica` refresh\n",
        "        ON proposal.snapshot_id = refresh.brid\n",
        "      WHERE proposal_id = {}\n",
        "  \"\"\"\n",
        "  query=sql.format(proposal_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "  snapshot_id = df.loc[0, 'snapshot_id']\n",
        "  start_date = df.loc[0, 'refresh_date']\n",
        "  order_cadence = df.loc[0, 'order_cadence']\n",
        "  # for now for end_date, we use later of (arrival_date + order_cadence) and today\n",
        "  # we should revisit this, for example, the discontinued items should be stop at some point\n",
        "  end_date = df.loc[0, 'arrival_date']\n",
        "  end_date = end_date + datetime.timedelta(int(order_cadence))\n",
        "  end_date = datetime.date.today() if end_date < datetime.date.today() else end_date\n",
        "\n",
        "  ##### Delete Existing Data #####\n",
        "  sql =\"\"\"\\\n",
        "      DELETE FROM `{}.analytics_lost_sales.tbl_lost_sales_prototype`\n",
        "      WHERE proposal_id = {}\n",
        "  \"\"\"\n",
        "  query=sql.format(project_id, proposal_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  ##### Delete Data From Previous Proposals #####\n",
        "  # for hypothetical case, we need to handle deletion differently\n",
        "  # this is because the start date of the simulation is not same across all products\n",
        "  sql =\"\"\"\\\n",
        "      DELETE FROM `{}.analytics_lost_sales.tbl_lost_sales_prototype` \n",
        "      WHERE date >= '{}'\n",
        "      AND analysis_case IN ('actual', 'model')\n",
        "      AND manufacturer_part_id IN\n",
        "        (SELECT DISTINCT manufacturer_part_id \n",
        "        FROM `wf-gcp-us-ae-ops-prod.buyfair_stream.vw_buyfair_supply_proposal`\n",
        "        WHERE proposal_id = {})\n",
        "  \"\"\"\n",
        "  query=sql.format(project_id, start_date, proposal_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  ##### Actual Case #####\n",
        "  print(\"working on actual case...\")\n",
        "  sql =\"\"\"\\\n",
        "      DECLARE param_proposal_id INT64 DEFAULT {};\n",
        "      DECLARE param_start_date DATE DEFAULT '{}';\n",
        "      DECLARE param_end_date DATE DEFAULT '{}';\n",
        "      CALL `{}.analytics_lost_sales.sp_lost_sales_actual_case_data_prep`\n",
        "      (param_proposal_id, param_start_date, param_end_date);\n",
        "  \"\"\"\n",
        "  query=sql.format(proposal_id, start_date, end_date, project_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  # fix some data types\n",
        "  df.receipt_qty = df.receipt_qty.astype(float)\n",
        "  df.demand_qty = df.demand_qty.astype(float)\n",
        "  df.wholesale_cost_per_unit = df.wholesale_cost_per_unit.astype(float)\n",
        "  df.revenue_per_unit = df.revenue_per_unit.astype(float)\n",
        "  df.date = pd.to_datetime(df.date)\n",
        "\n",
        "  df = df.sort_values(by=['manufacturer_part_id','date'])\n",
        "  # fill in missing values using linear interpolation\n",
        "  df.pickable_qty = df.pickable_qty.interpolate(limit_direction='both')\n",
        "  df.wholesale_cost_per_unit = df.wholesale_cost_per_unit.interpolate(limit_direction='both')\n",
        "  df.revenue_per_unit = df.revenue_per_unit.interpolate(limit_direction='both')\n",
        "  df['is_oos'] = np.where(df.pickable_qty <= 0, 1, 0)\n",
        "\n",
        "  tmp_table_name, fq_tmp_table_name = create_temp_table_from_df(project_id, df)\n",
        "\n",
        "  # insert data to real table\n",
        "  sql =\"\"\"\\\n",
        "      DECLARE param_fg_tmp_table_name STRING DEFAULT '{}';\n",
        "      CALL `{}.analytics_lost_sales.sp_lost_sales_actual_case_data_insert`(param_fg_tmp_table_name);\n",
        "  \"\"\"\n",
        "  query=sql.format(fq_tmp_table_name, project_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  drop_temp_table(client, dataset, tmp_table_name)\n",
        "\n",
        "  ##### Hypothetical Case #####\n",
        "  print(\"working on hypothetical case...\")\n",
        "  sql =\"\"\"\\\n",
        "      DECLARE param_proposal_id INT64 DEFAULT {};\n",
        "      DECLARE param_start_date DATE DEFAULT '{}';\n",
        "      DECLARE param_end_date DATE DEFAULT '{}';\n",
        "      CALL `{}.analytics_lost_sales.sp_lost_sales_hypothetical_case_data_prep`\n",
        "      (param_proposal_id, param_start_date, param_end_date);\n",
        "  \"\"\"\n",
        "  query=sql.format(proposal_id, start_date, end_date, project_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  # the following columns are duplicated to support hypothetical case 1 and 2\n",
        "  #     start_qty\n",
        "  #     order_qty\n",
        "  #     hypothetical_qty\n",
        "  # fix some data types\n",
        "  df.start_qty = df.start_qty.astype(float)\n",
        "  df['start_qty2'] = df.start_qty\n",
        "  df.other_receipt_qty = df.other_receipt_qty.astype(float)\n",
        "  df.order_qty = df.order_qty.astype(float)\n",
        "  df['order_qty2'] = df.order_qty\n",
        "  df.wholesale_cost_per_unit = df.wholesale_cost_per_unit.astype(float)\n",
        "  df.revenue_per_unit = df.revenue_per_unit.astype(float)\n",
        "  df.date = pd.to_datetime(df.date)\n",
        "  df.pickable_qty = df.pickable_qty.astype(float)\n",
        "  df.on_hand_qty = df.on_hand_qty.astype(float)\n",
        "\n",
        "  # fill missing data\n",
        "  df.pickable_qty = df.pickable_qty.interpolate(limit_direction='both')\n",
        "  df.on_hand_qty = df.on_hand_qty.interpolate(limit_direction='both')\n",
        "  df.wholesale_cost_per_unit = df.wholesale_cost_per_unit.interpolate(limit_direction='both')\n",
        "  df.revenue_per_unit = df.revenue_per_unit.interpolate(limit_direction='both')\n",
        "\n",
        "  # if products proposed but are not ordered or not arrived, simply use proposed qty and date it suppose to arrive\n",
        "  # once arrived, we need to distribute the proposed qty based on actual receipt qtys and dates\n",
        "  df['hypothetical_qty'] = np.where(df.proposal_receipt_total_qty == 0, df.proposal_qty, \n",
        "                                    1.0 * df.proposal_l1_qty * df.proposal_receipt_qty / df.proposal_receipt_total_qty)\n",
        "  df['hypothetical_qty2'] = np.where(df.proposal_receipt_total_qty == 0, df.proposal_qty2, \n",
        "                                    1.0 * df.proposal_l2_qty * df.proposal_receipt_qty / df.proposal_receipt_total_qty)\n",
        "\n",
        "  df.drop(columns=['proposal_receipt_qty', 'proposal_qty', 'proposal_qty2', 'proposal_l1_qty', 'proposal_l2_qty',\n",
        "                  'proposal_l4_qty', 'proposal_receipt_total_qty'], inplace=True)\n",
        "\n",
        "  # now let's separate the products into 2 groups (dataframes)\n",
        "  # 1 - products that don't exist before --> bleed down from refresh date\n",
        "  # 2 - products that do exist --> bleed down from impact date (this can be receipt date or expected arrival date)\n",
        "  #                                 then delete duplicated records\n",
        "  df_new = df[df.existing_product == 0].copy()\n",
        "  df_new.drop(columns=['on_hand_qty'], inplace=True)\n",
        "  df_new = bleed_down_hypothetical(df_new)\n",
        "\n",
        "  df_exist = df[df.existing_product == 1].copy()\n",
        "  # for each product, find the first date it is received\n",
        "  # backout a couple of days from the pickable date\n",
        "  df_exist_product = df_exist[df_exist.hypothetical_qty>0].groupby(['manufacturer_part_id'], as_index=False).agg(\n",
        "      {'date': 'min'})\n",
        "  df_exist_product.date = df_exist_product.date + datetime.timedelta(-2)\n",
        "  df_exist_product.rename(columns={'date': 'impact_date'}, inplace=True)\n",
        "  df_exist = pd.merge(df_exist,\n",
        "                      df_exist_product,\n",
        "                      left_on='manufacturer_part_id',\n",
        "                      right_on='manufacturer_part_id',\n",
        "                      how='inner')\n",
        "  df_exist = df_exist[df_exist.date >= df_exist.impact_date].copy()\n",
        "  # now initialize the start qty with on_hand_qty (note: not pickable_qty)\n",
        "  df_exist.start_qty = np.where(df_exist.date == df_exist.impact_date, df_exist.on_hand_qty, 0)\n",
        "  df_exist.start_qty2 = df_exist.start_qty\n",
        "  df_exist.drop(columns=['on_hand_qty', 'impact_date'], inplace=True)\n",
        "  df_exist = bleed_down_hypothetical(df_exist)\n",
        "\n",
        "  df = pd.concat([df_new, df_exist])\n",
        "\n",
        "  tmp_table_name, fq_tmp_table_name = create_temp_table_from_df(project_id, df)\n",
        "\n",
        "  # now delete older records basing on what we are about to insert\n",
        "  sql =\"\"\"\\\n",
        "      DELETE FROM `{}.analytics_lost_sales.tbl_lost_sales_prototype` pt \n",
        "      WHERE date >= '{}'\n",
        "      AND EXISTS(SELECT 1 FROM {} tmp\n",
        "        WHERE pt.analysis_case = tmp.analysis_case AND pt.date = CAST(tmp.date AS DATE) AND pt.manufacturer_part_id = tmp.manufacturer_part_id)\n",
        "  \"\"\"\n",
        "  query=sql.format(project_id, start_date, fq_tmp_table_name)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  # insert data to real table\n",
        "  sql =\"\"\"\\\n",
        "      DECLARE param_fg_tmp_table_name STRING DEFAULT '{}';\n",
        "      CALL `{}.analytics_lost_sales.sp_lost_sales_hypothetical_case_data_insert`(param_fg_tmp_table_name);\n",
        "  \"\"\"\n",
        "  query=sql.format(fq_tmp_table_name, project_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  drop_temp_table(client, dataset, tmp_table_name)\n",
        "\n",
        "  ###### Model Case #####\n",
        "  print(\"working on model case...\")\n",
        "  sql =\"\"\"\\\n",
        "      DECLARE param_snapshot_id INT64 DEFAULT {};\n",
        "      DECLARE param_proposal_id INT64 DEFAULT {};\n",
        "      DECLARE param_start_date DATE DEFAULT '{}';\n",
        "      DECLARE param_end_date DATE DEFAULT '{}';\n",
        "      CALL `{}.analytics_lost_sales.sp_lost_sales_model_case_data_prep`\n",
        "      (param_snapshot_id, param_proposal_id, param_start_date, param_end_date);\n",
        "  \"\"\"\n",
        "\n",
        "  query=sql.format(snapshot_id, proposal_id, start_date, end_date, project_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  # fix some data types\n",
        "  df.start_qty = df.start_qty.astype(float)\n",
        "  df.inbound_qty = df.inbound_qty.astype(float)\n",
        "  df.proposal_qty = df.proposal_qty.astype(float)\n",
        "  df.forecast_qty = df.forecast_qty.astype(float)\n",
        "  df.date = pd.to_datetime(df.date)\n",
        "  df.wholesale_cost_per_unit = df.wholesale_cost_per_unit.astype(float)\n",
        "  df.revenue_per_unit = df.revenue_per_unit.astype(float)\n",
        "  df.potential_lost_sales = df.potential_lost_sales.astype(float)\n",
        "  df.wholesale_cost_per_unit = df.wholesale_cost_per_unit.interpolate(limit_direction='both')\n",
        "  df.revenue_per_unit = df.revenue_per_unit.interpolate(limit_direction='both')\n",
        "\n",
        "  df = bleed_down_model(df)\n",
        "\n",
        "  tmp_table_name, fq_tmp_table_name = create_temp_table_from_df(project_id, df)\n",
        "\n",
        "  # insert data to real table\n",
        "  sql =\"\"\"\\\n",
        "      DECLARE param_fg_tmp_table_name STRING DEFAULT '{}';\n",
        "      CALL `{}.analytics_lost_sales.sp_lost_sales_model_case_data_insert`(param_fg_tmp_table_name);\n",
        "  \"\"\"\n",
        "  query=sql.format(fq_tmp_table_name, project_id)\n",
        "  df = client.query(query).to_dataframe()\n",
        "\n",
        "  drop_temp_table(client, dataset, tmp_table_name)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8BJYEmbLl0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "begin_time = datetime.datetime.now()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7xVwMTypN4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_id = \"wf-gcp-us-ae-buyfair-dev\"\n",
        "dataset = \"analytics_lost_sales\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7FO_Gf7TjGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "client = bigquery.Client(project=project_id)  \n",
        "sql =\"\"\"\\\n",
        "    SELECT *\n",
        "    FROM `wf-gcp-us-ae-ops-prod.buyfair_stream.vw_buyfair_supply_proposal_used`\n",
        "    ORDER BY snapshot_id\n",
        "\"\"\"\n",
        "query=sql.format()\n",
        "df = client.query(query).to_dataframe()\n",
        "proposals = df.proposal_id.tolist()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXZheZZSPcDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the proposal id and run the script by Ctrl+F10\n",
        "#proposals = [5799107, 5952237]\n",
        "#proposals = [5801358, 5904781]\n",
        "#proposals = [5756686, 5850428]\n",
        "proposals = [5852230, 5956819, 6099142]\n",
        "#proposals = [6099142]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EDF34BbJB2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a1979a17-9d44-42ef-ce7d-69d7b2a837c1"
      },
      "source": [
        "for proposal_id in proposals:\n",
        "  prototype_run(client, project_id, proposal_id)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------5852230-------\n",
            "working on actual case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.17s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "working on hypothetical case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.84s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "working on model case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.44s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------5956819-------\n",
            "working on actual case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.98s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "working on hypothetical case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.95s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "working on model case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.27s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------6099142-------\n",
            "working on actual case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.10s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "working on hypothetical case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.27s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "working on model case...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.56s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZsute5FLaNE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9bbc724-b3be-4357-bfa0-2f3cc7cc7688"
      },
      "source": [
        "print(datetime.datetime.now() - begin_time)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:21:47.957104\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}